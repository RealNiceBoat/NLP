{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBClassifier\nimport pickle\nfrom sklearn.multiclass import OneVsRestClassifier\ntrain = pd.read_csv('../input/til2020/TIL_NLP_train_dataset.csv', index_col='id')","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Ytrain = train[[\"outwear\", \"top\", \"trousers\", \"women dresses\", \"women skirts\"]].values","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf1 = TfidfVectorizer()\ntfidf1.fit(train['word_representation'])\nX_text1 = tfidf1.transform(train['word_representation']).toarray()\n\n#pickle.dump(tfidf1, open(\"tfidf_uni.pickle\", \"wb\"))\n\nXtrain1 = pd.DataFrame(X_text1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf2 = TfidfVectorizer(ngram_range=(1, 2))\ntfidf2.fit(train['word_representation'])\nX_text2 = tfidf2.transform(train['word_representation']).toarray()\nXtrain2 = pd.DataFrame(X_text2)\n\npickle.dump(tfidf2, open(\"tfidf_bi.pickle\", \"wb\"))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\n\ndef get_metrics(y_test, y_predicted):\n    # true positives / (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, average='micro')             \n    # true positives / (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, average='micro')\n    # harmonic mean of precision and recall\n    f1 = 2 * (precision * recall) / (precision + recall)\n    # true positives + true negatives/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return f1, precision, recall, accuracy","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = OneVsRestClassifier(XGBClassifier(n_estimators=100, random_state=0, tree_method='gpu_hist', gpu_id=0))\nmodel1.fit(Xtrain1, Ytrain)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"OneVsRestClassifier(estimator=XGBClassifier(base_score=None, booster=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, gamma=None,\n                                            gpu_id=0, importance_type='gain',\n                                            interaction_constraints=None,\n                                            learning_rate=None,\n                                            max_delta_step=None, max_depth=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=100, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            random_state=0, reg_alpha=None,\n                                            reg_lambda=None,\n                                            scale_pos_weight=None,\n                                            subsample=None,\n                                            tree_method='gpu_hist',\n                                            validate_parameters=None,\n                                            verbosity=None))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(model1, open('tfidf_uni.sav', 'wb'))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/til2020/TIL_NLP_test_dataset.csv', index_col='id')\nX_testtext1 = tfidf1.transform(test['word_representation']).toarray()\nXtest1 = pd.DataFrame(X_testtext1)\ny_pred_prob1 = model1.predict_proba(Xtest1)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model1","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = OneVsRestClassifier(XGBClassifier(n_estimators=100, random_state=0, tree_method='gpu_hist', gpu_id=0))\nmodel2.fit(Xtrain2, Ytrain)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"OneVsRestClassifier(estimator=XGBClassifier(base_score=None, booster=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, gamma=None,\n                                            gpu_id=0, importance_type='gain',\n                                            interaction_constraints=None,\n                                            learning_rate=None,\n                                            max_delta_step=None, max_depth=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=100, n_jobs=None,\n                                            num_parallel_tree=None,\n                                            random_state=0, reg_alpha=None,\n                                            reg_lambda=None,\n                                            scale_pos_weight=None,\n                                            subsample=None,\n                                            tree_method='gpu_hist',\n                                            validate_parameters=None,\n                                            verbosity=None))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(model2, open('tfidf_bi.sav', 'wb'))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_testtext2 = tfidf2.transform(test['word_representation']).toarray()\nXtest2 = pd.DataFrame(X_testtext2)\ny_pred_prob2 = model2.predict_proba(Xtest2)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred_prob2)","execution_count":13,"outputs":[{"output_type":"stream","text":"[[2.10789725e-01 9.82939124e-01 9.73868847e-01 1.07290718e-04\n  5.78892359e-04]\n [9.97117043e-01 9.99498487e-01 6.70479052e-03 7.46134960e-04\n  2.34530703e-03]\n [9.98438776e-01 1.08066425e-02 9.96100783e-01 1.46862396e-04\n  9.58087840e-05]\n ...\n [9.83061135e-01 2.01729294e-02 9.96654510e-01 3.56607197e-04\n  1.65813938e-02]\n [6.96623385e-01 9.95002329e-01 9.92879450e-01 8.17008840e-05\n  2.21126073e-04]\n [2.01649722e-02 4.47501335e-03 9.63162601e-01 1.87176731e-04\n  9.99941707e-01]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model2","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_thresh(Ytest, prob):\n    thresholds = []\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        y_pred_comb = [[1 if x > thresh else 0 for idx,x in enumerate(i) ] for i in prob]\n        res = get_metrics(Ytest, y_pred_comb)[0]\n        thresholds.append([thresh, res])\n        #print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    thresholds.sort(key=lambda x: x[1], reverse=True)\n    best_thresh = thresholds[0][0]\n    print(\"Best threshold: \", best_thresh)\n    return best_thresh","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/til2020/word_embeddings.pkl', 'rb') as f:\n    embeddings = pickle.load(f)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\nsentences = train[\"word_representation\"].values\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nprint(\"max sequence length:\", max(len(s) for s in sequences))","execution_count":17,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Found 4249 unique tokens.\nmax sequence length: 47\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=50)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            100,\n                            weights=[embedding_matrix],\n                            input_length=50,\n                            trainable=False)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, LSTM, GlobalMaxPool1D, Dropout\nfrom keras.layers import Bidirectional\nfrom keras.models import Model\n\nsequence_input = Input(shape=(50,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Bidirectional(LSTM(15, return_sequences=True))(embedded_sequences)\nx = GlobalMaxPool1D()(x) # consider all the h(t)s but only get 1 output\nx = Dropout(0.2)(x)\n# output = Dense(1, activation=\"sigmoid\")(x)\n#x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n#x = Bidirectional(LSTM(64, return_sequences=True))(x)\n#x = Attention(50)(x)\n#x = Dense(64, activation=\"relu\")(x)\n#x = Dropout(0.1)(x)\nx = Dense(5, activation=\"sigmoid\")(x)\n\nmodel = Model(sequence_input, x)\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[keras.metrics.Precision(), keras.metrics.Recall()])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmc = ModelCheckpoint('lstm.hdf5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training model...')\nr = model.fit(\n  data,\n  Ytrain,\n  batch_size=128,\n  epochs=200,\n  validation_split=0.1,\n  callbacks=[es, mc]\n)","execution_count":26,"outputs":[{"output_type":"stream","text":"Training model...\nTrain on 6642 samples, validate on 738 samples\nEpoch 1/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.6103 - precision_2: 0.5134 - recall_2: 0.4134 - val_loss: 0.5847 - val_precision_2: 0.5345 - val_recall_2: 0.3053\n\nEpoch 00001: val_loss improved from inf to 0.58468, saving model to lstm.hdf5\nEpoch 2/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.5529 - precision_2: 0.6458 - recall_2: 0.4262 - val_loss: 0.5315 - val_precision_2: 0.6927 - val_recall_2: 0.5038\n\nEpoch 00002: val_loss improved from 0.58468 to 0.53150, saving model to lstm.hdf5\nEpoch 3/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.4859 - precision_2: 0.7504 - recall_2: 0.5625 - val_loss: 0.4592 - val_precision_2: 0.7491 - val_recall_2: 0.6197\n\nEpoch 00003: val_loss improved from 0.53150 to 0.45922, saving model to lstm.hdf5\nEpoch 4/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.4128 - precision_2: 0.8281 - recall_2: 0.6710 - val_loss: 0.3874 - val_precision_2: 0.8953 - val_recall_2: 0.6803\n\nEpoch 00004: val_loss improved from 0.45922 to 0.38735, saving model to lstm.hdf5\nEpoch 5/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.3482 - precision_2: 0.8901 - recall_2: 0.7438 - val_loss: 0.3260 - val_precision_2: 0.9283 - val_recall_2: 0.7553\n\nEpoch 00005: val_loss improved from 0.38735 to 0.32596, saving model to lstm.hdf5\nEpoch 6/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.2949 - precision_2: 0.9168 - recall_2: 0.7971 - val_loss: 0.2790 - val_precision_2: 0.9370 - val_recall_2: 0.8000\n\nEpoch 00006: val_loss improved from 0.32596 to 0.27903, saving model to lstm.hdf5\nEpoch 7/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.2599 - precision_2: 0.9271 - recall_2: 0.8322 - val_loss: 0.2480 - val_precision_2: 0.9550 - val_recall_2: 0.8364\n\nEpoch 00007: val_loss improved from 0.27903 to 0.24795, saving model to lstm.hdf5\nEpoch 8/200\n6642/6642 [==============================] - 9s 1ms/step - loss: 0.2308 - precision_2: 0.9419 - recall_2: 0.8595 - val_loss: 0.2212 - val_precision_2: 0.9604 - val_recall_2: 0.8629\n\nEpoch 00008: val_loss improved from 0.24795 to 0.22117, saving model to lstm.hdf5\nEpoch 9/200\n6642/6642 [==============================] - 9s 1ms/step - loss: 0.2076 - precision_2: 0.9471 - recall_2: 0.8783 - val_loss: 0.2022 - val_precision_2: 0.9559 - val_recall_2: 0.8864\n\nEpoch 00009: val_loss improved from 0.22117 to 0.20222, saving model to lstm.hdf5\nEpoch 10/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.1904 - precision_2: 0.9536 - recall_2: 0.8917 - val_loss: 0.1847 - val_precision_2: 0.9668 - val_recall_2: 0.9045\n\nEpoch 00010: val_loss improved from 0.20222 to 0.18470, saving model to lstm.hdf5\nEpoch 11/200\n6642/6642 [==============================] - 9s 1ms/step - loss: 0.1769 - precision_2: 0.9577 - recall_2: 0.9014 - val_loss: 0.1735 - val_precision_2: 0.9653 - val_recall_2: 0.9061\n\nEpoch 00011: val_loss improved from 0.18470 to 0.17354, saving model to lstm.hdf5\nEpoch 12/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1629 - precision_2: 0.9627 - recall_2: 0.9121 - val_loss: 0.1625 - val_precision_2: 0.9694 - val_recall_2: 0.9129\n\nEpoch 00012: val_loss improved from 0.17354 to 0.16248, saving model to lstm.hdf5\nEpoch 13/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.1525 - precision_2: 0.9636 - recall_2: 0.9174 - val_loss: 0.1565 - val_precision_2: 0.9671 - val_recall_2: 0.9129\n\nEpoch 00013: val_loss improved from 0.16248 to 0.15648, saving model to lstm.hdf5\nEpoch 14/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1456 - precision_2: 0.9667 - recall_2: 0.9194 - val_loss: 0.1466 - val_precision_2: 0.9735 - val_recall_2: 0.9167\n\nEpoch 00014: val_loss improved from 0.15648 to 0.14656, saving model to lstm.hdf5\nEpoch 15/200\n6642/6642 [==============================] - 9s 1ms/step - loss: 0.1371 - precision_2: 0.9651 - recall_2: 0.9295 - val_loss: 0.1466 - val_precision_2: 0.9736 - val_recall_2: 0.9235\n\nEpoch 00015: val_loss did not improve from 0.14656\nEpoch 16/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.1318 - precision_2: 0.9661 - recall_2: 0.9283 - val_loss: 0.1356 - val_precision_2: 0.9760 - val_recall_2: 0.9235\n\nEpoch 00016: val_loss improved from 0.14656 to 0.13559, saving model to lstm.hdf5\nEpoch 17/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1237 - precision_2: 0.9719 - recall_2: 0.9335 - val_loss: 0.1298 - val_precision_2: 0.9700 - val_recall_2: 0.9318\n\nEpoch 00017: val_loss improved from 0.13559 to 0.12980, saving model to lstm.hdf5\nEpoch 18/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1208 - precision_2: 0.9672 - recall_2: 0.9380 - val_loss: 0.1331 - val_precision_2: 0.9603 - val_recall_2: 0.9341\n\nEpoch 00018: val_loss did not improve from 0.12980\nEpoch 19/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1176 - precision_2: 0.9714 - recall_2: 0.9343 - val_loss: 0.1245 - val_precision_2: 0.9745 - val_recall_2: 0.9280\n\nEpoch 00019: val_loss improved from 0.12980 to 0.12447, saving model to lstm.hdf5\nEpoch 20/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1129 - precision_2: 0.9710 - recall_2: 0.9413 - val_loss: 0.1192 - val_precision_2: 0.9724 - val_recall_2: 0.9341\n\nEpoch 00020: val_loss improved from 0.12447 to 0.11924, saving model to lstm.hdf5\nEpoch 21/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1093 - precision_2: 0.9703 - recall_2: 0.9428 - val_loss: 0.1151 - val_precision_2: 0.9763 - val_recall_2: 0.9356\n\nEpoch 00021: val_loss improved from 0.11924 to 0.11514, saving model to lstm.hdf5\nEpoch 22/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.1052 - precision_2: 0.9740 - recall_2: 0.9430 - val_loss: 0.1136 - val_precision_2: 0.9710 - val_recall_2: 0.9371\n\nEpoch 00022: val_loss improved from 0.11514 to 0.11363, saving model to lstm.hdf5\nEpoch 23/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.1007 - precision_2: 0.9747 - recall_2: 0.9468 - val_loss: 0.1114 - val_precision_2: 0.9732 - val_recall_2: 0.9364\n\nEpoch 00023: val_loss improved from 0.11363 to 0.11136, saving model to lstm.hdf5\nEpoch 24/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0975 - precision_2: 0.9748 - recall_2: 0.9472 - val_loss: 0.1104 - val_precision_2: 0.9687 - val_recall_2: 0.9379\n\nEpoch 00024: val_loss improved from 0.11136 to 0.11040, saving model to lstm.hdf5\nEpoch 25/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0949 - precision_2: 0.9748 - recall_2: 0.9493 - val_loss: 0.1086 - val_precision_2: 0.9679 - val_recall_2: 0.9371\n\nEpoch 00025: val_loss improved from 0.11040 to 0.10859, saving model to lstm.hdf5\nEpoch 26/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0944 - precision_2: 0.9759 - recall_2: 0.9488 - val_loss: 0.1072 - val_precision_2: 0.9718 - val_recall_2: 0.9402\n\nEpoch 00026: val_loss improved from 0.10859 to 0.10720, saving model to lstm.hdf5\nEpoch 27/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0932 - precision_2: 0.9726 - recall_2: 0.9507 - val_loss: 0.1060 - val_precision_2: 0.9672 - val_recall_2: 0.9394\n\nEpoch 00027: val_loss improved from 0.10720 to 0.10600, saving model to lstm.hdf5\nEpoch 28/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.0902 - precision_2: 0.9746 - recall_2: 0.9528 - val_loss: 0.1094 - val_precision_2: 0.9762 - val_recall_2: 0.9333\n\nEpoch 00028: val_loss did not improve from 0.10600\nEpoch 29/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0891 - precision_2: 0.9772 - recall_2: 0.9520 - val_loss: 0.1053 - val_precision_2: 0.9702 - val_recall_2: 0.9364\n\nEpoch 00029: val_loss improved from 0.10600 to 0.10530, saving model to lstm.hdf5\nEpoch 30/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0854 - precision_2: 0.9777 - recall_2: 0.9530 - val_loss: 0.1021 - val_precision_2: 0.9740 - val_recall_2: 0.9379\n","name":"stdout"},{"output_type":"stream","text":"\nEpoch 00030: val_loss improved from 0.10530 to 0.10209, saving model to lstm.hdf5\nEpoch 31/200\n6642/6642 [==============================] - 9s 1ms/step - loss: 0.0822 - precision_2: 0.9782 - recall_2: 0.9577 - val_loss: 0.1021 - val_precision_2: 0.9757 - val_recall_2: 0.9417\n\nEpoch 00031: val_loss improved from 0.10209 to 0.10206, saving model to lstm.hdf5\nEpoch 32/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0864 - precision_2: 0.9723 - recall_2: 0.9547 - val_loss: 0.1038 - val_precision_2: 0.9719 - val_recall_2: 0.9424\n\nEpoch 00032: val_loss did not improve from 0.10206\nEpoch 33/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0793 - precision_2: 0.9791 - recall_2: 0.9566 - val_loss: 0.0994 - val_precision_2: 0.9741 - val_recall_2: 0.9417\n\nEpoch 00033: val_loss improved from 0.10206 to 0.09936, saving model to lstm.hdf5\nEpoch 34/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.0780 - precision_2: 0.9789 - recall_2: 0.9580 - val_loss: 0.0983 - val_precision_2: 0.9735 - val_recall_2: 0.9462\n\nEpoch 00034: val_loss improved from 0.09936 to 0.09829, saving model to lstm.hdf5\nEpoch 35/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0741 - precision_2: 0.9786 - recall_2: 0.9615 - val_loss: 0.0990 - val_precision_2: 0.9734 - val_recall_2: 0.9409\n\nEpoch 00035: val_loss did not improve from 0.09829\nEpoch 36/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0735 - precision_2: 0.9805 - recall_2: 0.9599 - val_loss: 0.0988 - val_precision_2: 0.9668 - val_recall_2: 0.9485\n\nEpoch 00036: val_loss did not improve from 0.09829\nEpoch 37/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0736 - precision_2: 0.9798 - recall_2: 0.9622 - val_loss: 0.0972 - val_precision_2: 0.9735 - val_recall_2: 0.9447\n\nEpoch 00037: val_loss improved from 0.09829 to 0.09715, saving model to lstm.hdf5\nEpoch 38/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0705 - precision_2: 0.9813 - recall_2: 0.9628 - val_loss: 0.0955 - val_precision_2: 0.9727 - val_recall_2: 0.9462\n\nEpoch 00038: val_loss improved from 0.09715 to 0.09546, saving model to lstm.hdf5\nEpoch 39/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0730 - precision_2: 0.9798 - recall_2: 0.9579 - val_loss: 0.0958 - val_precision_2: 0.9720 - val_recall_2: 0.9462\n\nEpoch 00039: val_loss did not improve from 0.09546\nEpoch 40/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.0679 - precision_2: 0.9815 - recall_2: 0.9649 - val_loss: 0.0967 - val_precision_2: 0.9735 - val_recall_2: 0.9447\n\nEpoch 00040: val_loss did not improve from 0.09546\nEpoch 41/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0675 - precision_2: 0.9820 - recall_2: 0.9635 - val_loss: 0.0946 - val_precision_2: 0.9749 - val_recall_2: 0.9432\n\nEpoch 00041: val_loss improved from 0.09546 to 0.09462, saving model to lstm.hdf5\nEpoch 42/200\n6642/6642 [==============================] - 9s 1ms/step - loss: 0.0650 - precision_2: 0.9831 - recall_2: 0.9652 - val_loss: 0.0983 - val_precision_2: 0.9713 - val_recall_2: 0.9470\n\nEpoch 00042: val_loss did not improve from 0.09462\nEpoch 43/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0637 - precision_2: 0.9817 - recall_2: 0.9667 - val_loss: 0.0953 - val_precision_2: 0.9691 - val_recall_2: 0.9508\n\nEpoch 00043: val_loss did not improve from 0.09462\nEpoch 44/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0636 - precision_2: 0.9825 - recall_2: 0.9659 - val_loss: 0.0963 - val_precision_2: 0.9720 - val_recall_2: 0.9485\n\nEpoch 00044: val_loss did not improve from 0.09462\nEpoch 45/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0627 - precision_2: 0.9835 - recall_2: 0.9659 - val_loss: 0.0954 - val_precision_2: 0.9727 - val_recall_2: 0.9455\n\nEpoch 00045: val_loss did not improve from 0.09462\nEpoch 46/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.0600 - precision_2: 0.9850 - recall_2: 0.9675 - val_loss: 0.0948 - val_precision_2: 0.9749 - val_recall_2: 0.9432\n\nEpoch 00046: val_loss did not improve from 0.09462\nEpoch 47/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0589 - precision_2: 0.9833 - recall_2: 0.9688 - val_loss: 0.0968 - val_precision_2: 0.9691 - val_recall_2: 0.9500\n\nEpoch 00047: val_loss did not improve from 0.09462\nEpoch 48/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0602 - precision_2: 0.9830 - recall_2: 0.9675 - val_loss: 0.0957 - val_precision_2: 0.9720 - val_recall_2: 0.9470\n\nEpoch 00048: val_loss did not improve from 0.09462\nEpoch 49/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0579 - precision_2: 0.9840 - recall_2: 0.9687 - val_loss: 0.0930 - val_precision_2: 0.9736 - val_recall_2: 0.9500\n\nEpoch 00049: val_loss improved from 0.09462 to 0.09300, saving model to lstm.hdf5\nEpoch 50/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0584 - precision_2: 0.9840 - recall_2: 0.9696 - val_loss: 0.0946 - val_precision_2: 0.9728 - val_recall_2: 0.9492\n\nEpoch 00050: val_loss did not improve from 0.09300\nEpoch 51/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0576 - precision_2: 0.9836 - recall_2: 0.9701 - val_loss: 0.0962 - val_precision_2: 0.9735 - val_recall_2: 0.9447\n\nEpoch 00051: val_loss did not improve from 0.09300\nEpoch 52/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.0561 - precision_2: 0.9835 - recall_2: 0.9705 - val_loss: 0.0922 - val_precision_2: 0.9728 - val_recall_2: 0.9470\n\nEpoch 00052: val_loss improved from 0.09300 to 0.09224, saving model to lstm.hdf5\nEpoch 53/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0535 - precision_2: 0.9846 - recall_2: 0.9715 - val_loss: 0.0953 - val_precision_2: 0.9691 - val_recall_2: 0.9508\n\nEpoch 00053: val_loss did not improve from 0.09224\nEpoch 54/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0610 - precision_2: 0.9844 - recall_2: 0.9656 - val_loss: 0.0937 - val_precision_2: 0.9713 - val_recall_2: 0.9500\n\nEpoch 00054: val_loss did not improve from 0.09224\nEpoch 55/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0543 - precision_2: 0.9834 - recall_2: 0.9725 - val_loss: 0.0952 - val_precision_2: 0.9728 - val_recall_2: 0.9500\n\nEpoch 00055: val_loss did not improve from 0.09224\nEpoch 56/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0519 - precision_2: 0.9851 - recall_2: 0.9727 - val_loss: 0.0942 - val_precision_2: 0.9706 - val_recall_2: 0.9500\n\nEpoch 00056: val_loss did not improve from 0.09224\nEpoch 57/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0508 - precision_2: 0.9851 - recall_2: 0.9741 - val_loss: 0.0948 - val_precision_2: 0.9743 - val_recall_2: 0.9462\n\nEpoch 00057: val_loss did not improve from 0.09224\nEpoch 58/200\n6642/6642 [==============================] - 11s 2ms/step - loss: 0.0502 - precision_2: 0.9867 - recall_2: 0.9719 - val_loss: 0.0949 - val_precision_2: 0.9683 - val_recall_2: 0.9500\n\nEpoch 00058: val_loss did not improve from 0.09224\nEpoch 59/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0499 - precision_2: 0.9853 - recall_2: 0.9734 - val_loss: 0.0959 - val_precision_2: 0.9720 - val_recall_2: 0.9462\n\nEpoch 00059: val_loss did not improve from 0.09224\nEpoch 60/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0483 - precision_2: 0.9845 - recall_2: 0.9756 - val_loss: 0.0974 - val_precision_2: 0.9655 - val_recall_2: 0.9530\n\nEpoch 00060: val_loss did not improve from 0.09224\nEpoch 61/200\n6642/6642 [==============================] - 10s 2ms/step - loss: 0.0496 - precision_2: 0.9851 - recall_2: 0.9751 - val_loss: 0.0943 - val_precision_2: 0.9728 - val_recall_2: 0.9485\n\nEpoch 00061: val_loss did not improve from 0.09224\nEpoch 62/200\n6642/6642 [==============================] - 10s 1ms/step - loss: 0.0464 - precision_2: 0.9869 - recall_2: 0.9759 - val_loss: 0.0948 - val_precision_2: 0.9677 - val_recall_2: 0.9538\n\nEpoch 00062: val_loss did not improve from 0.09224\nEpoch 00062: early stopping\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.models import load_weights\nmodel.load_weights('/kaggle/working/lstm.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sentences = test[\"word_representation\"].values\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\nXtest_emb = pad_sequences(test_sequences, maxlen=50)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predembprob = model.predict(Xtest_emb)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_combprob = 0.33*(y_pred_prob1+y_pred_prob2+y_predembprob)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#threshcomb = calculate_thresh(Ytest, y_pred_combprob)\ny_predcomb = [[1 if x > 0.4 else 0 for idx,x in enumerate(i) ] for i in y_pred_combprob]","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/til2020/NLP_submission_example.csv')\nsubmission[[\"outwear\", \"top\", \"trousers\", \"women dresses\", \"women skirts\"]] = y_predcomb\nsubmission.to_csv('submission.csv', index=False)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}